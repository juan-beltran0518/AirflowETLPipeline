# Airflow Sales ETL Pipeline

Pipeline ETL de demostraciÃ³n con Apache Airflow para procesar datos de ventas y generar reportes por categorÃ­a. Incluye validaciÃ³n de datos, transformaciÃ³n con pandas y generaciÃ³n automatizada de reportes agregados usando arquitectura containerizada con Docker.

## ğŸš€ CaracterÃ­sticas

- **Pipeline ETL automatizado** con Apache Airflow
- **ValidaciÃ³n de datos** de entrada
- **TransformaciÃ³n de datos** con pandas
- **GeneraciÃ³n de reportes** por categorÃ­as
- **ContainerizaciÃ³n** con Docker
- **OrquestaciÃ³n de tareas** con dependencias

## ğŸ“‹ Prerrequisitos

- Docker y Docker Compose
- Python 3.8+
- Al menos 4GB de RAM disponible

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

1. **Clona el repositorio:**
   ```bash
   git clone https://github.com/juan-beltran0518/AirflowETLPipeline.git
   cd airflow-demo
   ```

2. **Crea el archivo de variables de entorno:**
   ```bash
   echo "AIRFLOW_UID=$(id -u)" > .env
   ```

3. **Inicia los servicios con Docker Compose:**
   ```bash
   docker-compose up -d
   ```

4. **Accede a la interfaz web de Airflow:**
   - URL: http://localhost:8080
   - Usuario: `admin`
   - ContraseÃ±a: `admin`

## ğŸ“Š Estructura del Proyecto

```
airflow-demo/
â”œâ”€â”€ dags/                          # DAGs de Airflow
â”‚   â””â”€â”€ demo_csv_to_report.py     # Pipeline principal ETL
â”œâ”€â”€ data/                          # Datos de entrada y salida
â”‚   â”œâ”€â”€ sales.csv                 # Datos de ventas (entrada)
â”‚   â””â”€â”€ report_sales_by_category.csv # Reporte generado (salida)
â”œâ”€â”€ logs/                          # Logs de Airflow
â”œâ”€â”€ plugins/                       # Plugins personalizados
â”œâ”€â”€ docker-compose.yml            # ConfiguraciÃ³n de Docker
â””â”€â”€ README.md                     # Este archivo
```

## ğŸ”„ Uso del Pipeline

### Ejecutar el DAG manualmente:

1. Ve a la interfaz web de Airflow (http://localhost:8080)
2. Busca el DAG `demo_csv_to_report`
3. Activa el DAG con el toggle
4. Haz clic en "Trigger DAG" para ejecutarlo

### Funcionamiento del Pipeline:

1. **`validate_input`**: Verifica que existe el archivo de datos de entrada
2. **`transform_to_report`**: 
   - Lee los datos de ventas
   - Calcula el revenue (precio Ã— cantidad)
   - Agrupa por categorÃ­a y calcula mÃ©tricas
   - Genera el reporte CSV
3. **`list_outputs`**: Verifica que el reporte se generÃ³ correctamente

## ğŸ“ˆ Datos de Ejemplo

El pipeline procesa datos de ventas con la siguiente estructura:

**Entrada (`sales.csv`):**
```csv
order_id,date,product,category,unit_price,qty
1001,2025-01-05,Keyboard,Peripherals,20,2
1002,2025-01-05,Mouse,Peripherals,10,3
```

**Salida (`report_sales_by_category.csv`):**
```csv
category,total_qty,total_revenue,orders
Computers,2,1350,2
Displays,3,530,2
Peripherals,16,306,6
```

## ğŸ›‘ Detener los Servicios

```bash
docker-compose down
```

## ğŸ”§ PersonalizaciÃ³n

Para modificar el pipeline:

1. Edita `dags/demo_csv_to_report.py`
2. Los cambios se reflejarÃ¡n automÃ¡ticamente en Airflow
3. Agrega nuevos archivos CSV en la carpeta `data/`

## ğŸ“ Logs

Los logs se almacenan en la carpeta `logs/` y estÃ¡n organizados por:
- DAG ID
- Run ID
- Task ID


